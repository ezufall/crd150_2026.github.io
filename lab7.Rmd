---
title: "Lab 7: Mapping Open Data"
subtitle: <font size="4">CRD 150 - Quantitative Methods in Community Research</font>
author: Professor Noli Brazil
date: May 14, 2025
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    theme: cosmo
    code_folding: show
    self_contained: false
---


<style>
p.comment {
background-color: #DBDBDB;
padding: 10px;
border: 1px solid black;
margin-left: 25px;
border-radius: 5px;
font-style: normal;
}

.figure {
   margin-top: 20px;
   margin-bottom: 20px;
}

h1.title {
  font-weight: bold;
  font-family: Arial;  
}

h2.title {
  font-family: Arial;  
}

</style>


<style type="text/css">
#TOC {
  font-size: 13px;
  font-family: Arial;
}
</style>


\

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message = FALSE)
```

In our journey into spatial R, we've been exclusively working with polygon data. In this guide you will learn how to handle and descriptively analyze point data.  More broadly, we will cover how to access and clean data from Open Data portals. The objectives of the guide are as follows 

1. Learn how to download data from an Open Data portal
2. Learn how to read point data into R
3. Learn how to put latitude and longitude data on a map
4. Learn how to put street addresses on a map
5. Learn how to map point data

To achieve these objectives, we will examine the spatial distribution of homeless encampments in the City of Los Angeles using [311](https://lacity.gov/myla311) data downloaded from the city's [open data portal](https://data.lacity.org/).

This lab guide follows closely and supplements the material presented in Chapters 2.4, 2.5, 4.2 and 7 in the textbook [Geocomputation with R](https://geocompr.robinlovelace.net/) (GWR).

<p class="comment">**Assignment 7 is due by 12:00 pm, May 21st on Canvas.**  See [here](https://crd150.github.io/hw_guidelines.html) for assignment guidelines.  You must submit an `.Rmd` file and its associated `.html` file. Name the files: yourLastName_firstInitial_asgn07. For example: brazil_n_asgn07.</p>


<div style="margin-bottom:25px;">
</div>
## **Open up an R Markdown file**
\

Download the [Lab template](https://raw.githubusercontent.com/crd150/data/master/labtemplate.Rmd) into an appropriate folder on your hard drive (preferably, a folder named 'Lab 7'), open it in R Studio, and type and run your code there.  The template is also located on Canvas under Files.  Change the title ("Lab 7") and insert your name and date. Don't change anything else inside the YAML (the stuff at the top in between the `---`).  For a rundown on the use of R Markdown, see the [assignment guidelines](https://crd150.github.io/hw_guidelines.html).

<div style="margin-bottom:25px;">
</div>
## **Installing and loading packages**
\

You’ll need to install the following package in R. We’ll talk about what this package provides as its relevant functions come up in the guide.

```{r message = FALSE, warning = FALSE, eval = FALSE}
install.packages("tidygeocoder")
```

You’ll need to load the following packages. Unlike installing, you will always need to load packages whenever you start a new R session. You’ll also always need to use `library()` in your R Markdown file.

```{r message = FALSE, warning=FALSE}
library(sf)
library(tidyverse)
library(tidycensus)
library(tigris)
library(tmap)
library(rmapshaper)
library(tidygeocoder)
```


<div style="margin-bottom:25px;">
</div>
## **Read in census tract data**
\

We will need to bring in census tract polygon features and racial composition data from the 2015-2019 American Community Survey using the Census API and keep tracts within Los Angeles city boundaries using a clip.  The code for accomplishing these tasks is below.  We won't go through each line of code in detail because we've covered all of these operations and functions in prior labs.  I've embedded comments within the code that briefly explain what each chunk is doing, but go back to prior guides (or RDS/GWR) if you need further help.  

```{r warning=FALSE, results = "hide", message=FALSE}
# Bring in census tract data with spatial geometry 
ca.tracts <- get_acs(geography = "tract", 
              year = 2019,
              variables = c(tpop = "B01003_001", tpopr = "B03002_001", 
                            white = "B03002_003", black = "B03002_004",
                             asian = "B03002_006", hisp = "B03002_012"),
              state = "CA",
              survey = "acs5",
              output = "wide",
              geometry = TRUE)

# Calculate percent race/ethnicity, rename variables, and keep essential vars.
ca.tracts <- ca.tracts %>% 
  mutate(pwhite = whiteE/tpoprE, pasian = asianE/tpoprE, 
              pblack = blackE/tpoprE, phisp = hispE/tpoprE) %>%
  rename(tpop = tpopE) %>%
  select(GEOID,tpop, pwhite, pasian, pblack, phisp)

# Bring in city boundary data
pl <- places(state = "CA", cb = TRUE)

# Keep LA city
la.city <- pl %>%
            filter(NAME == "Los Angeles")

#Clip tracts using LA boundary
la.city.tracts <- ms_clip(target = ca.tracts, 
                          clip = la.city, 
                          remove_slivers = TRUE)
```

Make sure to take a look at the final data object.

```{r}
glimpse(la.city.tracts)
```

<div style="margin-bottom:25px;">
</div>
## **Point data from an open data portal**
\

We will be examining the spatial distribution of homeless encampments in Los Angeles City. We will download homeless encampment data from Los Angeles' Open Data portal. The data represent homeless encampment locations in 2019 as reported through the City's [311 system](https://www.lacity.org/myla311).  To download the data from LA's open data portal, follow these steps:

1. Navigate to the  [Los Angeles City Open Data portal](https://data.lacity.org/A-Well-Run-City/MyLA311-Service-Request-Data-2019/pvft-t768). Note the record layout for the dataset further down the page.
2. Click on *Actions* and then *Query data*.   This will bring up the  data in an excel style worksheet.
2. You'll find that there are over one million 311 requests in 2019.  Rather than bring all of these requests into R, let's just filter for homeless encampments.  To do this, click on the bottom arrow next to the box *Select a column to filter...*.  and select *Request Type* from the  pull down menu. Then type in *Homeless Encampment* in the text box that appears (*Search...*). Hit the Apply button located at the bottom of the screen.
4. Click on *Export* and select *CSV*.  Download the file into an appropriate folder on your hard drive (where your Lab RMarkdown resides). Name the file "homeless311_la_2019".

Read the file into R using the function `read_csv()`. Make sure your current working directory is pointing to the appropriate folder (use `getwd()` to see the current directory and `setwd()` to change it).


```{r eval = FALSE}
homeless311.df <- read_csv("homeless311_la_2019.csv")
```

If you had trouble downloading the file, I uploaded it onto GitHub. Read the file into R using the following code.  


```{r warning = FALSE, message = FALSE}
homeless311.df <- read_csv("https://raw.githubusercontent.com/crd150/data/master/homeless311_la_2019.csv")
```

Whenever you bring a dataset into R, always look at it.  We examine *homeless311.df* using `glimpse()`

```{r}
glimpse(homeless311.df)
```

Let's bring in a csv file containing the street addresses of [homeless shelters and services in Los Angeles County](https://www.lahsa.org/portal/apps/find-a-shelter/adults) (as of 2019), which I also downloaded from Los Angeles' open data portal.  I uploaded the file onto Github so you don't have to download it from the portal like we did above with encampments. The file is also located on Canvas (Week 7 - Lab).

```{r warning = FALSE, message = FALSE}
shelters.df <- read_csv("https://raw.githubusercontent.com/crd150/data/master/Homeless_Shelters_and_Services.csv")
```

Take a look at the data

```{r}
glimpse(shelters.df)
```


<div style="margin-bottom:25px;">
</div>
## **Putting points on a map**
\

Notice that the data we brought in are not spatial. They are regular tibbles, not **sf** objects. They're class is not **sf**.

```{r}
class(homeless311.df)
class(shelters.df)
```

And they don’t have geometry as a column.

```{r}
names(homeless311.df)
names(shelters.df)
```


In order to convert these nonspatial objects into spatial objects, we need to geolocate them.  To do this, use the geographic coordinates (longitude and latitude) to place points on a map. The file *homeless311.df* has longitude and latitude coordinates, so we have all the information we need to convert it to a spatial object.  

We will use the function `st_as_sf()` to create a point **sf** object out of *homeless311.df* using the  geographic coordinates.  Geographic coordinates are in the form of a longitude and latitude, where longitude is your X coordinate and spans East/West and latitude is your Y coordinate and spans North/South. The function `st_as_sf()` requires you to specify the longitude and latitude of each point using the `coords =` argument, which are conveniently stored in the variables *Longitude* and *Latitude* in *homeless311.df*. We need to take out any observations with missing values `NA` for either *Longitude* or *Latitude*.  Do this by using the function `drop_na()`

```{r}
homeless311.df <- homeless311.df %>%
                drop_na(Longitude, Latitude)
```

You also need to establish the Coordinate Reference System (CRS) using the `crs =` argument. To establish a CRS, you will need to specify the Geographic Coordinate System (so you know where your points are on Earth), which encompasses the datum and ellipse, and a Projection (a way of putting points in 2 dimensions or on a flat map). 

```{r}
homeless311.sf <- st_as_sf(homeless311.df, 
                           coords = c("Longitude", "Latitude"), 
                           crs = "+proj=longlat +datum=NAD83 +ellps=WGS84")
```

For `crs =`, we establish the projection `+proj=`, datum `+datum=` and ellipse `+ellps`.  Because we have latitude and longitude, `+proj=` is established as `longlat`. We use NAD83 for datum and WGS84 ellipse. The most common datum and ellipsoid combinations are listed in Figure 1 in the Coordinate_Reference_Systems.pdf document on Canvas (Files - Other Resources).  For the purposes of this lab and assignment 7 (and in your final project if you choose to use point data), use the crs specified above.

Note that the CRS should have only have one space in between `+proj=longlat`, `+datum=WGS84` and `+ellps=WGS84`.  And you should have no spaces anywhere else.  Be careful about this because if you introduce spaces elsewhere you will get an incorrect CRS (and R will not give you an error to let you know). Also note that the longitude will always go before the latitude in `coords =`.

Now we can map encampments over Los Angeles City tracts using our new best friend `tm_shape()`, whom we met in [Lab 5](https://crd150.github.io/lab5.html#Mapping_in_R). Because encampments are points, we need to use `tm_dots()` to map them. We color these points in red using the `fill =` argument.

```{r eval = FALSE}
tm_shape(la.city.tracts) +  
  tm_polygons() +
tm_shape(homeless311.sf) +  
  tm_dots(fill="red")
```


<div style="margin-bottom:25px;">
</div>
## **Street Addresses**
\

Often you will get point data that won’t have longitude/X and latitude/Y coordinates but instead have street addresses. The process of going from address to X/Y coordinates is known as geocoding. 

To demonstrate geocoding, type in your home street address, city and state inside the quotes below.  If your home address isn't working, use the address for [Woodstock Pizza](https://woodstocksdavis.com/) (238 G St, Davis, CA). We are saving our address in an object we named *myaddress.df*.

```{r eval = FALSE}
myaddress.df  <- tibble(street = "238 G St", 
                        city = "Davis", 
                        state = "CA")
```

This creates a tibble with your street, city and state saved in three variables. To geocode addresses to longitude and latitude, use the function `geocode()` which is a part of the **tidygeocoder** package.  Use `geocode()` as follows

```{r eval = FALSE}
myaddress.df <- geocode(myaddress.df, 
                        street = street, 
                        city = city, 
                        state = state, 
                        method = "osm")
```

Here, we specify street, city and state variables, which are conveniently named *street*, *city*, and *state* in *myaddress.df*.  The argument `method = "osm"` specifies the geocoder used to map addresses to longitude/latitude locations. In the above case `'osm'` uses the free geocoder provided by OpenStreetMap to find the address.  Think of a geocoder as R going to the OpenStreetMap [website](https://www.openstreetmap.org/#map=4/38.01/-95.84), searching for each address,  plucking the latitude and longitude of the address, and saving it into a tibble named *myaddress.df*.  Other geocoders include Google Maps (not free) and the  [US Census](https://geocoding.geo.census.gov/) (free). If you're having trouble geocoding using "osm", try "census" for `method = `. 


If you view this object, you'll find the latitude *lat* and longitude *long* attached as columns. 

```{r eval = FALSE}
glimpse(myaddress.df)
```

Convert this to an **sf** object using the function `st_as_sf()` like we did above with the encampments.

```{r eval = FALSE}
myaddress.sf <- st_as_sf(myaddress.df, 
                         coords = c("long", "lat"),
                         crs = "+proj=longlat +datum=NAD83 +ellps=WGS84")
```

Type in `tmap_mode("view")` and then [map](https://crd150.github.io/lab5.html#Interactive_maps) *myaddress.sf*.  Zoom into the point.  Did it get your home address correct?

<br>

The file *shelters.df* contains no latitude and longitude data, so we need to convert the street addresses contained in the variables *addrln1*, *city* and *state*.  View the data to verify. 

Use the function `geocode()` like we did above.  We use `method = "osm"` to indicate we want to use the OSM API to geocode the addresses for us (as opposed to other geocoders that are available). The process may take a few minutes so be patient.

```{r}
shelters.geo <- geocode(shelters.df, 
                        street = addrln1, 
                        city = city, 
                        state = state, 
                        method = "osm")
```

Look at the column names.

```{r}
names(shelters.geo)
```

And take a look at the data

```{r}
glimpse(shelters.geo)
```

We see the latitudes and longitudes are attached to the variables *lat* and *long*, respectively.  Notice that not all the addresses were successfully geocoded.

```{r}
summary(shelters.geo$lat)
```

Several shelters received an `NA`.  This is likely because the addresses are not correct, has errors, or are not fully specified.  You'll have to manually fix these issues, which becomes time consuming if you have a really large data set.  For the purposes of this lab, let's just discard these, but in practice, make sure to double check your address data (See the document Geocoding_Best_Practices.pdf in the Other Resources folder on Canvas for best practices for cleaning address data). We use the `drop_na()` function to filter out the NAs.

```{r}
shelters.geo <- shelters.geo %>%
                drop_na(lat, long)
```

Convert latitude and longitude data into spatial points using the function `st_as_sf()` like we did above for homeless encampments.

```{r warning = FALSE, message = FALSE}
shelters.sf <- st_as_sf(shelters.geo, 
                        coords = c("long", "lat"), 
                        crs = "+proj=longlat +datum=NAD83 +ellps=WGS84")
```

Now, let's map shelters (in blue) and encampments (in red) on top of the tracts.

```{r}
tm_shape(la.city.tracts) +
  tm_polygons() +
tm_shape(homeless311.sf) +  
  tm_dots(fill= "red") +
tm_shape(shelters.sf) +  
  tm_dots(fill="blue")
```


<div style="margin-bottom:25px;">
</div>
## **Mapping point patterns**
\


Other than mapping their locations (e.g. pin or dot map), how else can we visually present point locations?  When working with neighborhoods, we can examine point distributions by summing up the number of points in each neighborhood. 

To do this, we can use the function `aggregate()`, which is part of the **sf** package.

```{r}
hcamps_agg <- aggregate(x = homeless311.sf["SRNumber"], 
                        by = la.city.tracts, 
                        FUN = "length")
```

* The first argument `x =` is the **sf** points you want to count, in our case *homeless311.sf*.  
* You then include in bracket and quotes the unique ID for each point, in our case "SRNumber". 
* The next argument `by =` is the object you want to sum the points for, in our case Los Angeles census tracts *la.city.tracts*.  
* Finally,`FUN = "length"` tells R to sum up the number of points in each tract (as opposed to taking the max, min, median, etc.).

Take a look at the object we created.

```{r}
glimpse(hcamps_agg)
```

The variable *SRNumber* gives us the number of points in each of Los Angeles' tracts.  Notice that there are missing values.

```{r}
summary(hcamps_agg)
```

This is because there are tracts that have no homeless encampments.  Convert these NA values to 0 using the `replace_na()` function within the `mutate()` function.

```{r}
hcamps_agg <- hcamps_agg %>%
  mutate(SRNumber = replace_na(SRNumber,0))
```

The function `replace_na()` tells R to replace any NAs found in the variable *SRNumber* with the number 0.  

Next, we save the variable *SRNumber* from *hcamps_agg* into our main dataset *la.city.tracts* by creating a new variable *hcamps* within `mutate()`.

```{r warning=FALSE, message=FALSE}
la.city.tracts <- la.city.tracts %>%
            mutate(hcamps = hcamps_agg$SRNumber)
```


We can map the count of encampments by census tract, but counts do not take into consideration exposure.  In this case, tracts that are larger in size or greater in population may have more encampments (a concept we covered in Handout 5). Instead, let's calculate the number of homeless encampments per 1,000 residents.


```{r warning=FALSE, message=FALSE}
la.city.tracts <- la.city.tracts %>%
            mutate(hcamppop = (hcamps/tpop)*1000)
```

The above code creates the variable *hcamppop* in *la.city.tracts*, which represents the number of homeless encampments per 1,000 residents.  Note that there are tracts with 0 population sizes, so a division of 0 when you created the variable *hcamppop* will yield an Inf value, which we covered in [Lab 1](https://crd150.github.io/lab1.html#Numeric).  Let's convert *hcamppop* to NA for tracts with Inf values using the `na_if()` function.

```{r warning=FALSE, message=FALSE}
la.city.tracts <- la.city.tracts %>%
            mutate(hcamppop = na_if(hcamppop, Inf))
```

Now we can create a presentable choropleth map of homeless encampments per 1,000 residents for neighborhoods in the City of Los Angeles.

```{r}
la.city.tracts %>%
  tm_shape(unit = "mi") +
  tm_polygons(fill = "hcamppop", col_alpha = 0,
              fill.scale = tm_scale(style = "quantile",
                                    values = "reds"),
              fill.legend = tm_legend(title = "Encampments per 1k pop",
                                      frame = FALSE))  +
  tm_title("Homeless encampments in Los Angeles Tracts, 2019") +
  tm_scalebar(position = tm_pos_in("center", "bottom")) +
  tm_compass(type = "4star", 
             position = tm_pos_in("left", "bottom")) +
  tm_layout(scale = 0.6, 
            frame = FALSE) 
```  

Create maps of racial/ethnic composition and see if there are visual associations between race/ethnicity and homeless encampments in Los Angeles neighborhoods.

<div style="margin-bottom:25px;">
</div>
## **Assignment 7**
\
                       
Download and open the [Assignment 7 R Markdown Script](https://raw.githubusercontent.com/crd150/data/master/yourLastName_firstInitial_asgn07.Rmd). The script can also be found on Canvas (Files - Week 7 - Assignment). Any response requiring a data analysis task  must be supported by code you generate to produce your result. (Just examining your various objects in the “Environment” section of R Studio is insufficient—you must use scripted commands.). 
                       
<br>
                         
1. In this question, you will be exploring the spatial distribution of Airbnb listings in the City of Oakland. [Airbnb](https://press.airbnb.com/about-us/) involves renting an entire home, a room or a shared room, typically from a private citizen. The data were downloaded from Airbnb's public use [data site](http://insideairbnb.com/get-the-data.html). Download the zip file *assignment7files.zip* from Canvas (Files - Week 7 - Assignment). Save the files in an appropriate folder. Unzip it and place the files in the same folder as your Assignment 7 RMarkdown file. The folder contains the file *oak_tracts.shp*,  which contains Oakland City tracts with their population sizes downloaded from the 2015-2019 American Community Survey, the csv file *oakland_airbnb_dec2020.csv*, which contains Airbnb locations in December 2020, and the file *station_info.csv*, which contains the addresses of [BART](https://www.bart.gov/system-map) station locations in Oakland. The record layout for *oakland_airbnb_dec2020.csv* can be found [here](https://raw.githubusercontent.com/crd150/data/master/oakland_airbnb_record_layout.txt). 


a. Read the file *oakland_airbnb_dec2020.csv* into R. Create a point dataset of Airbnb listings using their longitudes and latitudes.  (2 points)
b. Read the file *oak_tracts.shp* into R. Create a variable that represents the number of Airbnb listings in each neighborhood. (3 points) 
c. Create a presentation-ready choropleth map of the number of Airbnb listings per 1,000 residents. (2 points)
d. Are neighborhoods with a large presence of Airbnb listings near BART stations, which is the Bay Area's main public rapid-transit system? Read the file *station_info.csv* into R. Geocode the BART stations using their street addresses. (1.5 points)
e. Map the BART station point locations onto the choropleth map you created in (c). Label the points by the name of the BART Station. Based on a visual assessment of the map, are neighborhoods with a large presence of Airbnb listings near BART stations? (1.5 points)
                       
<br>
                         
2. The purpose of this question is to increase your exposure to the world of open data. In this exercise, you will explore a city's open data portal, answering questions about one of its data sets and examining the data set in R.  Let's do this for your final project community. If you haven't already selected one, pick a potential candidate from the [eligible list](https://crd150.github.io/eligible.html). 

a. Find your final project's open data portal. Provide the name of the community and the link to its portal. If your community does not have an open data portal, pick **one** of the following cities. Which city/county did you choose?

+ New York, NY [(link)](https://opendata.cityofnewyork.us/)
+ San Francisco, CA [(link)](https://datasf.org/opendata/)
+ Los Angeles, CA [(link)](https://data.lacity.org/)
+ Chicago, IL [(link)](https://data.cityofchicago.org/)

b. Find a dataset that looks interesting to you and can be downloaded and brought into R.  Answer the following questions about this dataset.

* Describe the dataset you selected. What are the variables, the year(s) captured by  the data, and the units of observations? (2 points)
* Are you able to manipulate the data online? This includes filtering rows and selecting columns. (1 point)
* Are you able to create visualizations online with the data using graphs and charts? If so, what kinds of graphs and charts? (1 point)
* Are you able to map the data online? If so, what types of maps? (1 point)
* Is the data set accompanied by a codebook with definitions of terms, variables and fields? (1 point)
* Bring the data set into R. Create either a single presentation-ready table, chart or graph that we covered in [Lab 4](https://crd150.github.io/lab4.html) or a single presentation-ready map that we covered in [Lab 5](https://crd150.github.io/lab5.html) or Lab 7. Explain what your graphic is showing us. You must show your code with comments to get full points for this question. (4 points)


***

<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International License</a>.


Website created and maintained by [Noli Brazil](https://nbrazil.faculty.ucdavis.edu/)
